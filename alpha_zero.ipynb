{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdf0945-fd71-47db-b638-3ad3269a653c",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "This is a proof of concept on how AlphaZero can be implemented on top of TorchRL. \n",
    "\n",
    "We will apply this technique on CliffWalking-v0 environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd8cc51-a251-4258-b834-76dd62ea60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "\n",
    "from torchrl.modules import QValueActor\n",
    "\n",
    "from torchrl.envs import GymEnv, TransformedEnv, Compose, DTypeCastTransform, StepCounter\n",
    "\n",
    "from torchrl.objectives import DQNLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36cae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb754c-1cd4-4254-91fa-ecb95724ee28",
   "metadata": {},
   "source": [
    "# QValue Network\n",
    "\n",
    "Lets first create a QValue network. QValue networks provide an initial value for each action when we explore a node for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2116d9-f8ed-4e5a-9df5-6ee2992baee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakyath/.local/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/prakyath/.local/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([48]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_q_value(num_observation, num_action, action_space):\n",
    "    net = torch.nn.Linear(num_observation, num_action)\n",
    "    qvalue_module = QValueActor(net, in_keys=[\"observation\"], action_space=action_space)\n",
    "    return qvalue_module\n",
    "\n",
    "\n",
    "env = TransformedEnv(\n",
    "    GymEnv(\"CliffWalking-v0\"),\n",
    "    Compose(\n",
    "        DTypeCastTransform(dtype_in=torch.long, dtype_out=torch.float32, in_keys=[\"observation\"]), \n",
    "        StepCounter(),\n",
    "    )\n",
    ")\n",
    "qvalue_module = make_q_value(env.observation_spec[\"observation\"].shape[-1], env.action_spec.shape[-1], env.action_spec)\n",
    "qvalue_module(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ffd66c-5b71-4a9e-b506-4235e07ce5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakyath/.local/lib/python3.10/site-packages/torchrl/objectives/dqn.py:176: UserWarning: You did not provide a delay_value argument for <class 'torchrl.objectives.dqn.DQNLoss'>. Currently (v0.3) the default for delay_value is `False` but as of v0.4 it will be `True`. Make sure to adapt your code depending on your preferred configuration. To remove this warning, indicate the value of delay_value in your script.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_module = DQNLoss(qvalue_module, action_space=env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857b0b9a-5e08-4b2d-84d2-5c10191a2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts.tensordict_map import TensorDictMap\n",
    "from mcts.mcts_policy import SimulatedSearchPolicy, MctsPolicy, UpdateTreeStrategy, AlphaZeroExpansionStrategy, PucbSelectionPolicy, SimulatedAlphaZeroSearchPolicy\n",
    "\n",
    "tree = TensorDictMap([\"observation\", \"step_count\"])\n",
    "\n",
    "\n",
    "policy = SimulatedAlphaZeroSearchPolicy(\n",
    "    policy=MctsPolicy(\n",
    "        expansion_strategy=AlphaZeroExpansionStrategy(value_module=qvalue_module, tree=tree),\n",
    "        selection_strategy=PucbSelectionPolicy(),\n",
    "    ),\n",
    "    tree_updater=UpdateTreeStrategy(tree),\n",
    "    env=env,\n",
    "    num_simulation=10,\n",
    "    max_steps=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abfdff8-fb8b-4119-b4f3-be0e253a08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors import SyncDataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46343be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_collecter = SyncDataCollector(lambda: env, policy, total_frames = 1_000_000, frames_per_batch = 10_000)\n",
    "print('done')\n",
    "from torchrl.trainers import Trainer\n",
    "Trainer(collector=data_collecter, loss_module=loss_module,  optimizer=torch.optim.Adam(qvalue_module.parameters(), lr=1e-3)).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a703f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

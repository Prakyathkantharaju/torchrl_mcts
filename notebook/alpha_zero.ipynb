{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdf0945-fd71-47db-b638-3ad3269a653c",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "This is a proof of concept on how AlphaZero can be implemented on top of TorchRL. \n",
    "\n",
    "We will apply this technique on CliffWalking-v0 environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd8cc51-a251-4258-b834-76dd62ea60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "\n",
    "from torchrl.modules import QValueActor\n",
    "\n",
    "from torchrl.envs import GymEnv, TransformedEnv, Compose, DTypeCastTransform\n",
    "\n",
    "from torchrl.objectives import DQNLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb754c-1cd4-4254-91fa-ecb95724ee28",
   "metadata": {},
   "source": [
    "# QValue Network\n",
    "\n",
    "Lets first create a QValue network. QValue networks provide an initial value for each action when we explore a node for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2116d9-f8ed-4e5a-9df5-6ee2992baee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majid/miniconda3/envs/torchrl_mcts/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/majid/miniconda3/envs/torchrl_mcts/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([48]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_q_value(num_observation, num_action, action_space):\n",
    "    net = torch.nn.Linear(num_observation, num_action)\n",
    "    qvalue_module = QValueActor(net, in_keys=[\"observation\"], action_space=action_space)\n",
    "    return qvalue_module\n",
    "\n",
    "\n",
    "env = TransformedEnv(\n",
    "    GymEnv(\"CliffWalking-v0\"),\n",
    "    Compose(\n",
    "        DTypeCastTransform(dtype_in=torch.long, dtype_out=torch.float32, in_keys=[\"observation\"])\n",
    "    )\n",
    ")\n",
    "qvalue_module = make_q_value(env.observation_spec[\"observation\"].shape[-1], env.action_spec.shape[-1], env.action_spec)\n",
    "qvalue_module(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ffd66c-5b71-4a9e-b506-4235e07ce5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majid/miniconda3/envs/torchrl_mcts/lib/python3.10/site-packages/torchrl/objectives/dqn.py:176: UserWarning: You did not provide a delay_value argument for <class 'torchrl.objectives.dqn.DQNLoss'>. Currently (v0.3) the default for delay_value is `False` but as of v0.4 it will be `True`. Make sure to adapt your code depending on your preferred configuration. To remove this warning, indicate the value of delay_value in your script.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_module = DQNLoss(qvalue_module, action_space=env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857b0b9a-5e08-4b2d-84d2-5c10191a2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts.tensordict_map import TensorDictMap\n",
    "from mcts.mcts_policy import SimulatedSearchPolicy, MctsPolicy, UpdateTreeStrategy, AlphaZeroExpansionStrategy, PucbSelectionPolicy\n",
    "\n",
    "tree = TensorDictMap(\"observation\")\n",
    "\n",
    "policy = SimulatedSearchPolicy(\n",
    "    policy=MctsPolicy(\n",
    "        expansion_strategy=AlphaZeroExpansionStrategy(value_module=qvalue_module, tree=tree),\n",
    "        selection_strategy=PucbSelectionPolicy(),\n",
    "    ),\n",
    "    tree_updater=UpdateTreeStrategy(tree),\n",
    "    env=env,\n",
    "    num_simulation=5,\n",
    "    max_steps=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3abfdff8-fb8b-4119-b4f3-be0e253a08b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_simulation\n",
      "forward\n",
      "simulation-0\n",
      "action_value\n",
      "q_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "n_sa: [0. 0. 0. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action: 2\n",
      "action_value\n",
      "q_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "n_sa: [0. 0. 0. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action: 2\n",
      "action_value\n",
      "q_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "n_sa: [0. 0. 0. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action: 2\n",
      "updated... [0 0 1 0]\n",
      "q_sa: [-0.04422648 -0.03890638 -3.         -0.14129654]\n",
      "n_sa: [0. 0. 1. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 0 1 0]\n",
      "q_sa: [-0.04422648 -0.03890638 -2.5        -0.14129654]\n",
      "n_sa: [0. 0. 2. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 0 1 0]\n",
      "q_sa: [-0.04422648 -0.03890638 -2.         -0.14129654]\n",
      "n_sa: [0. 0. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "1\n",
      "simulation-1\n",
      "action_value\n",
      "q_sa: [-0.04422648 -0.03890638 -2.         -0.14129654]\n",
      "n_sa: [0. 0. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.08252774 -0.07260029 -1.9804289  -0.26366293]\n",
      "action: 1\n",
      "action_value\n",
      "q_sa: [-0.04422648 -0.03890638 -2.         -0.14129654]\n",
      "n_sa: [0. 0. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.08252774 -0.07260029 -1.9804289  -0.26366293]\n",
      "action: 1\n",
      "action_value\n",
      "q_sa: [-0.04422648 -0.03890638 -2.         -0.14129654]\n",
      "n_sa: [0. 0. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.08252774 -0.07260029 -1.9804289  -0.26366293]\n",
      "action: 1\n",
      "updated... [0 1 0 0]\n",
      "q_sa: [-4.4226483e-02 -3.0000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [0. 1. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 1 0 0]\n",
      "q_sa: [-4.4226483e-02 -2.5000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [0. 2. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 1 0 0]\n",
      "q_sa: [-4.4226483e-02 -2.0000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [0. 3. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "1\n",
      "simulation-2\n",
      "action_value\n",
      "q_sa: [-4.4226483e-02 -2.0000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [0. 3. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-9.8392643e-02 -2.0001192e+02 -1.9723223e+00 -3.1434876e-01]\n",
      "action: 0\n",
      "action_value\n",
      "q_sa: [-0.10092085 -0.02470846 -0.11477355 -0.16039589]\n",
      "n_sa: [0. 0. 0. 0.]\n",
      "p_sa: [-0.10092085 -0.02470846 -0.11477355 -0.16039589]\n",
      "action_value: [-0.10092085 -0.02470846 -0.11477355 -0.16039589]\n",
      "action: 1\n",
      "action_value\n",
      "q_sa: [-0.09738857 -0.00902127  0.11961678 -0.17599708]\n",
      "n_sa: [0. 0. 0. 0.]\n",
      "p_sa: [-0.09738857 -0.00902127  0.11961678 -0.17599708]\n",
      "action_value: [-0.09738857 -0.00902127  0.11961678 -0.17599708]\n",
      "action: 2\n",
      "updated... [1 0 0 0]\n",
      "q_sa: [-1.0200000e+02 -2.0000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [1. 3. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 1 0 0]\n",
      "q_sa: [-1.0092085e-01 -1.0100000e+02 -1.1477355e-01 -1.6039589e-01]\n",
      "n_sa: [0. 1. 0. 0.]\n",
      "p_sa: [-0.10092085 -0.02470846 -0.11477355 -0.16039589]\n",
      "action_value: [-0.10092085 -0.02470846 -0.11477355 -0.16039589]\n",
      "updating done!\n",
      "updated... [0 0 1 0]\n",
      "q_sa: [-9.7388566e-02 -9.0212673e-03 -1.0000000e+02 -1.7599708e-01]\n",
      "n_sa: [0. 0. 1. 0.]\n",
      "p_sa: [-0.09738857 -0.00902127  0.11961678 -0.17599708]\n",
      "action_value: [-0.09738857 -0.00902127  0.11961678 -0.17599708]\n",
      "updating done!\n",
      "3\n",
      "simulation-3\n",
      "action_value\n",
      "q_sa: [-1.0200000e+02 -2.0000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [1. 3. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-102.02925   -200.01286     -1.9701047   -0.3282143]\n",
      "action: 3\n",
      "action_value\n",
      "q_sa: [-1.0200000e+02 -2.0000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [1. 3. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-102.02925   -200.01286     -1.9701047   -0.3282143]\n",
      "action: 3\n",
      "action_value\n",
      "q_sa: [-1.0200000e+02 -2.0000000e+02 -2.0000000e+00 -1.4129654e-01]\n",
      "n_sa: [1. 3. 3. 0.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-102.02925   -200.01286     -1.9701047   -0.3282143]\n",
      "action: 3\n",
      "updated... [0 0 0 1]\n",
      "q_sa: [-102. -200.   -2.   -3.]\n",
      "n_sa: [1. 3. 3. 1.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 0 0 1]\n",
      "q_sa: [-102.  -200.    -2.    -2.5]\n",
      "n_sa: [1. 3. 3. 2.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 0 0 1]\n",
      "q_sa: [-102. -200.   -2.   -2.]\n",
      "n_sa: [1. 3. 3. 3.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "3\n",
      "simulation-4\n",
      "action_value\n",
      "q_sa: [-102. -200.   -2.   -2.]\n",
      "n_sa: [1. 3. 3. 3.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-102.034966  -200.01538     -1.9642682   -2.0558524]\n",
      "action: 2\n",
      "action_value\n",
      "q_sa: [-102. -200.   -2.   -2.]\n",
      "n_sa: [1. 3. 3. 3.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-102.034966  -200.01538     -1.9642682   -2.0558524]\n",
      "action: 2\n",
      "action_value\n",
      "q_sa: [-102. -200.   -2.   -2.]\n",
      "n_sa: [1. 3. 3. 3.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-102.034966  -200.01538     -1.9642682   -2.0558524]\n",
      "action: 2\n",
      "updated... [0 0 1 0]\n",
      "q_sa: [-102.   -200.     -2.25   -2.  ]\n",
      "n_sa: [1. 3. 4. 3.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 0 1 0]\n",
      "q_sa: [-102.  -200.    -2.2   -2. ]\n",
      "n_sa: [1. 3. 5. 3.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "updated... [0 0 1 0]\n",
      "q_sa: [-102. -200.   -2.   -2.]\n",
      "n_sa: [1. 3. 6. 3.]\n",
      "p_sa: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "action_value: [-0.04422648 -0.03890638  0.09039502 -0.14129654]\n",
      "updating done!\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "res = policy(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba4a4f4-71e8-45d9-833a-cae089d3a467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree._dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2482ddfe-9b1b-4a9e-834b-b0c36c16bfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-102., -200.,   -2.,   -2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"q_sa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a877bc8-567a-4f1d-969c-47991b331a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0442, -0.0389,  0.0904, -0.1413])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"p_sa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aea3801-1aee-48af-b8f6-ae2146c60d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-102.0399, -200.0175,   -1.9767,   -2.0637])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"action_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b62e3c8-ab53-4078-bbe9-b1818c67fa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25222f02-914d-44cd-9c3f-5fab5aec8bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
